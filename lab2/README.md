# Lab 2 â€“ Fine-Tuning Transformer Models for NLP Tasks  
School of Electrical and Computer Engineering, NTUA | Neural Networks & Deep Learning Lab | Spring 2024

## Summary

This lab introduces fine-tuning of pre-trained transformer-based language models using the Hugging Face `transformers` library. The tasks include text classification on user reviews, natural language inference, and grammatical acceptability assessment using pretrained models like BERT and RoBERTa.

## Key Objectives

- Understand the distinction between pre-training and fine-tuning in NLP models
- Utilize Hugging Face pipelines for various text classification tasks
- Fine-tune models for sentiment analysis and text quality assessment
- Perform inference using models trained on CoLA and NLI datasets

## Technologies Used

- Python 3.x
- Hugging Face Transformers
- PyTorch
- Google Colab
- Datasets: IMDb reviews, CoLA, RoBERTa-MNLI
